{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4683f6d0-fc02-4c26-85b6-c91be4026ae3",
   "metadata": {},
   "source": [
    "# GNSS data processing script\n",
    "\n",
    "**Note: This notebook is not reproducible by default** because the source data paths direct to a large database. It is meant to give readers a sense about how the provided GNSS records (`Kaskawulsh_v2_2018-mm-dd_to_2018-mm-dd_GPS.csv`) were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736bcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy import interpolate\n",
    "import math\n",
    "import glob\n",
    "from pyproj import transform, CRS\n",
    "from math import degrees, atan2\n",
    "# from scipy.stats import linregress\n",
    "import scipy.stats\n",
    "from netCDF4 import Dataset, num2date\n",
    "import calendar\n",
    "from scipy.io import loadmat\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "def convert_time(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "      \n",
    "    return \"%d:%02d:%02d\" % (hour, minutes, seconds)\n",
    "\n",
    "def custom_mean(df):\n",
    "    return df.mean(skipna=False)\n",
    "\n",
    "def rsquared(x, y):\n",
    "    \"\"\" Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def truncate(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "horizontal_error = 0.12\n",
    "vertical_error = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read Lower data\n",
    "lower_path='/Users/willkochtitzky/Projects/Kaskawulsh/GPS_data/lower_cleaned.csv'\n",
    "#read in PPP corrected data (from NR CAN)\n",
    "#read lower data\n",
    "lower_pos_data = pd.read_csv(lower_path)#,usecols=['DIR','HGT(m)','YEAR-MM-DD','HR:MN:SS.SS','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)'])\n",
    "\n",
    "lower_pos_data['date'] = pd.to_datetime(lower_pos_data['YEAR-MM-DD']+'/'+lower_pos_data['HR:MN:SS.SS'],format='%Y-%m-%d/%H:%M:%S.%f')\n",
    "lower_pos_data['day_date'] = pd.to_datetime(lower_pos_data['YEAR-MM-DD'],format='%Y-%m-%d')\n",
    "lower_pos_data = lower_pos_data.drop(columns=['DIR','YEAR-MM-DD','HR:MN:SS.SS'])\n",
    "\n",
    "# lower_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']] = lower_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']].apply(pd.to_numeric)#change the data types\n",
    "lower_pos_data = lower_pos_data.sort_values(by=['date'])#sort the data\n",
    "\n",
    "#filter out good data to inspect the bad data later\n",
    "lower_pos_data_bad_data = lower_pos_data[(lower_pos_data['SDLAT(95%)']>horizontal_error) & (lower_pos_data['SDLON(95%)']>horizontal_error) & (lower_pos_data['SDHGT(95%)']>vertical_error)]  \n",
    "\n",
    "lower_pos_data = lower_pos_data[lower_pos_data['SDLAT(95%)']<horizontal_error]\n",
    "lower_pos_data = lower_pos_data[lower_pos_data['SDLON(95%)']<horizontal_error]\n",
    "lower_pos_data = lower_pos_data[lower_pos_data['SDHGT(95%)']<vertical_error]\n",
    "\n",
    "lower_pos_data['year'] = lower_pos_data['date'].dt.year\n",
    "lower_pos_data['day'] = lower_pos_data['date'].dt.dayofyear\n",
    "lower_pos_data = lower_pos_data.reset_index() #reset the index for the calculations ahead, this is crucial to make sure your counter is right!\n",
    "\n",
    "#calculate how much time has occured since 1/1/00 so that you can figure out how much time is between observations\n",
    "pos_sec_since = []\n",
    "jan1_2007 = datetime(2007,1,1)\n",
    "for i in range(0,len(lower_pos_data)):\n",
    "    pos_sec_since.append((lower_pos_data['date'][i]-jan1_2007).total_seconds())\n",
    "lower_pos_data['sec_since']=pos_sec_since\n",
    "\n",
    "lower_pos_data_daily = lower_pos_data.set_index('date').groupby(pd.Grouper(freq='d')).mean() #calculate daily data\n",
    "lower_pos_data_daily = lower_pos_data_daily.reset_index() #reset the index after you claculate daily data\n",
    "\n",
    "distance=[float('nan')]\n",
    "obs_duration_days=[float('nan')]\n",
    "for i in range(1,len(lower_pos_data_daily)):\n",
    "    distance.append(np.sqrt((lower_pos_data_daily['UTM_EASTING'][i]-lower_pos_data_daily['UTM_EASTING'][i-1])**2+(lower_pos_data_daily['UTM_NORTHING'][i]-lower_pos_data_daily['UTM_NORTHING'][i-1])**2))   \n",
    "    obs_duration_days.append((lower_pos_data_daily['sec_since'][i]-lower_pos_data_daily['sec_since'][i-1])/3600/24)   \n",
    "lower_pos_data_daily['Distance']=distance\n",
    "lower_pos_data_daily['obs_duration_days']=obs_duration_days\n",
    "lower_pos_data_daily[['Distance','obs_duration_days']] = lower_pos_data_daily[['Distance','obs_duration_days']].apply(pd.to_numeric)#change the data types\n",
    "lower_pos_data_daily['Velocity_m_per_d']=lower_pos_data_daily['Distance']/lower_pos_data_daily['obs_duration_days']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550af9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read middle data\n",
    "#middle_path='/Users/willkochtitzky/Projects/Kaskawulsh/GPS_data/Middle_all_data.csv'\n",
    "middle_path='/Users/willkochtitzky/Projects/Kaskawulsh/GPS_data/middle_cleaned.csv'\n",
    "#read in PPP corrected data (from NR CAN)\n",
    "middle_pos_data = pd.read_csv(middle_path)#,usecols=['DIR','HGT(m)','YEAR-MM-DD','HR:MN:SS.SS','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)'])\n",
    "\n",
    "middle_pos_data['date'] = pd.to_datetime(middle_pos_data['YEAR-MM-DD']+'/'+middle_pos_data['HR:MN:SS.SS'],format='%Y-%m-%d/%H:%M:%S.%f')\n",
    "middle_pos_data['day_date'] = pd.to_datetime(middle_pos_data['YEAR-MM-DD'],format='%Y-%m-%d')\n",
    "middle_pos_data = middle_pos_data.drop(columns=['DIR','YEAR-MM-DD','HR:MN:SS.SS'])\n",
    "\n",
    "# middle_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']] = middle_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']].apply(pd.to_numeric)#change the data types\n",
    "middle_pos_data = middle_pos_data.sort_values(by=['date'])#sort the data\n",
    "\n",
    "#filter out good data to inspect the bad data later\n",
    "middle_pos_data_bad_data = middle_pos_data[(middle_pos_data['SDLAT(95%)']>horizontal_error) & (middle_pos_data['SDLON(95%)']>horizontal_error) & (middle_pos_data['SDHGT(95%)']>vertical_error)]  \n",
    "\n",
    "middle_pos_data = middle_pos_data[middle_pos_data['SDLAT(95%)']<horizontal_error]\n",
    "middle_pos_data = middle_pos_data[middle_pos_data['SDLON(95%)']<horizontal_error]\n",
    "middle_pos_data = middle_pos_data[middle_pos_data['SDHGT(95%)']<vertical_error]\n",
    "\n",
    "middle_pos_data['year'] = middle_pos_data['date'].dt.year\n",
    "middle_pos_data['day'] = middle_pos_data['date'].dt.dayofyear\n",
    "middle_pos_data = middle_pos_data.reset_index() #reset the index for the calculations ahead, this is crucial to make sure your counter is right!\n",
    "\n",
    "#calculate how much time has occured since 1/1/00 so that you can figure out how much time is between observations\n",
    "pos_sec_since = []\n",
    "jan1_2007 = datetime(2007,1,1)\n",
    "for i in range(0,len(middle_pos_data)):\n",
    "    pos_sec_since.append((middle_pos_data['date'][i]-jan1_2007).total_seconds())\n",
    "middle_pos_data['sec_since']=pos_sec_since\n",
    "\n",
    "middle_pos_data_daily = middle_pos_data.set_index('date').groupby(pd.Grouper(freq='d')).mean() #calculate daily data\n",
    "middle_pos_data_daily = middle_pos_data_daily.reset_index() #reset the index after you claculate daily data\n",
    "\n",
    "distance=[float('nan')]\n",
    "obs_duration_days=[float('nan')]\n",
    "for i in range(1,len(middle_pos_data_daily)):\n",
    "    distance.append(np.sqrt((middle_pos_data_daily['UTM_EASTING'][i]-middle_pos_data_daily['UTM_EASTING'][i-1])**2+(middle_pos_data_daily['UTM_NORTHING'][i]-middle_pos_data_daily['UTM_NORTHING'][i-1])**2))   \n",
    "    obs_duration_days.append((middle_pos_data_daily['sec_since'][i]-middle_pos_data_daily['sec_since'][i-1])/3600/24)   \n",
    "middle_pos_data_daily['Distance']=distance\n",
    "middle_pos_data_daily['obs_duration_days']=obs_duration_days\n",
    "middle_pos_data_daily[['Distance','obs_duration_days']] = middle_pos_data_daily[['Distance','obs_duration_days']].apply(pd.to_numeric)#change the data types\n",
    "middle_pos_data_daily['Velocity_m_per_d']=middle_pos_data_daily['Distance']/middle_pos_data_daily['obs_duration_days']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca80ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Upper data\n",
    "upper_path='/Users/willkochtitzky/Projects/Kaskawulsh/GPS_data/upper_cleaned.csv'\n",
    "#read in PPP corrected data (from NR CAN)\n",
    "upper_pos_data = pd.read_csv(upper_path)#,usecols=['DIR','HGT(m)','YEAR-MM-DD','HR:MN:SS.SS','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)'])\n",
    "upper_pos_data['date'] = pd.to_datetime(upper_pos_data['YEAR-MM-DD']+'/'+upper_pos_data['HR:MN:SS.SS'],format='%Y-%m-%d/%H:%M:%S.%f')\n",
    "upper_pos_data['day_date'] = pd.to_datetime(upper_pos_data['YEAR-MM-DD'],format='%Y-%m-%d')\n",
    "upper_pos_data = upper_pos_data.drop(columns=['DIR','YEAR-MM-DD','HR:MN:SS.SS'])\n",
    "\n",
    "# upper_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']] = upper_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']].apply(pd.to_numeric)#change the data types\n",
    "upper_pos_data = upper_pos_data.sort_values(by=['date'])#sort the data\n",
    "\n",
    "#filter out good data to inspect the bad data later\n",
    "upper_pos_data_bad_data = upper_pos_data[(upper_pos_data['SDLAT(95%)']>horizontal_error) & (upper_pos_data['SDLON(95%)']>horizontal_error) & (upper_pos_data['SDHGT(95%)']>vertical_error)]  \n",
    "\n",
    "upper_pos_data = upper_pos_data[upper_pos_data['SDLAT(95%)']<horizontal_error]\n",
    "upper_pos_data = upper_pos_data[upper_pos_data['SDLON(95%)']<horizontal_error]\n",
    "upper_pos_data = upper_pos_data[upper_pos_data['SDHGT(95%)']<vertical_error]\n",
    "\n",
    "upper_pos_data['year'] = upper_pos_data['date'].dt.year\n",
    "upper_pos_data['day'] = upper_pos_data['date'].dt.dayofyear\n",
    "upper_pos_data = upper_pos_data.reset_index() #reset the index for the calculations ahead, this is crucial to make sure your counter is right!\n",
    "\n",
    "#calculate how much time has occured since 1/1/00 so that you can figure out how much time is between observations\n",
    "pos_sec_since = []\n",
    "jan1_2007 = datetime(2007,1,1)\n",
    "for i in range(0,len(upper_pos_data)):\n",
    "    pos_sec_since.append((upper_pos_data['date'][i]-jan1_2007).total_seconds())\n",
    "upper_pos_data['sec_since']=pos_sec_since\n",
    "\n",
    "upper_pos_data_daily = upper_pos_data.set_index('date').groupby(pd.Grouper(freq='d')).mean() #calculate daily data\n",
    "upper_pos_data_daily = upper_pos_data_daily.reset_index() #reset the index after you claculate daily data\n",
    "\n",
    "distance=[float('nan')]\n",
    "obs_duration_days=[float('nan')]\n",
    "for i in range(1,len(upper_pos_data_daily)):\n",
    "    distance.append(np.sqrt((upper_pos_data_daily['UTM_EASTING'][i]-upper_pos_data_daily['UTM_EASTING'][i-1])**2+(upper_pos_data_daily['UTM_NORTHING'][i]-upper_pos_data_daily['UTM_NORTHING'][i-1])**2))   \n",
    "    obs_duration_days.append((upper_pos_data_daily['sec_since'][i]-upper_pos_data_daily['sec_since'][i-1])/3600/24)   \n",
    "upper_pos_data_daily['Distance']=distance\n",
    "upper_pos_data_daily['obs_duration_days']=obs_duration_days\n",
    "upper_pos_data_daily[['Distance','obs_duration_days']] = upper_pos_data_daily[['Distance','obs_duration_days']].apply(pd.to_numeric)#change the data types\n",
    "upper_pos_data_daily['Velocity_m_per_d']=upper_pos_data_daily['Distance']/upper_pos_data_daily['obs_duration_days']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8022f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Arm data\n",
    "arm_path='/Users/willkochtitzky/Projects/Kaskawulsh/GPS_data/arm_cleaned.csv'\n",
    "#read in PPP corrected data (from NR CAN)\n",
    "arm_pos_data = pd.read_csv(arm_path)#,usecols=['DIR','HGT(m)','YEAR-MM-DD','HR:MN:SS.SS','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)'])\n",
    "arm_pos_data['date'] = pd.to_datetime(arm_pos_data['YEAR-MM-DD']+'/'+arm_pos_data['HR:MN:SS.SS'],format='%Y-%m-%d/%H:%M:%S.%f')\n",
    "arm_pos_data['day_date'] = pd.to_datetime(arm_pos_data['YEAR-MM-DD'],format='%Y-%m-%d')\n",
    "arm_pos_data = arm_pos_data.drop(columns=['DIR','YEAR-MM-DD','HR:MN:SS.SS'])\n",
    "\n",
    "# arm_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']] = arm_pos_data[['HGT(m)','UTM_EASTING','UTM_NORTHING','GDOP','RMSC(m)','RMSP(m)','SDLAT(95%)','SDLON(95%)','SDHGT(95%)','SDCLK(95%)','SDTZD(95%)']].apply(pd.to_numeric)#change the data types\n",
    "arm_pos_data = arm_pos_data.sort_values(by=['date'])#sort the data\n",
    "\n",
    "#filter out good data to inspect the bad data later\n",
    "arm_pos_data_bad_data = arm_pos_data[(arm_pos_data['SDLAT(95%)']>horizontal_error) & (arm_pos_data['SDLON(95%)']>horizontal_error) & (arm_pos_data['SDHGT(95%)']>vertical_error)]\n",
    "\n",
    "arm_pos_data = arm_pos_data[arm_pos_data['SDLAT(95%)']<horizontal_error]\n",
    "arm_pos_data = arm_pos_data[arm_pos_data['SDLON(95%)']<horizontal_error]\n",
    "arm_pos_data = arm_pos_data[arm_pos_data['SDHGT(95%)']<vertical_error]\n",
    "\n",
    "arm_pos_data['year'] = arm_pos_data['date'].dt.year\n",
    "arm_pos_data['day'] = arm_pos_data['date'].dt.dayofyear\n",
    "arm_pos_data = arm_pos_data.reset_index() #reset the index for the calculations ahead, this is crucial to make sure your counter is right!\n",
    "\n",
    "#calculate how much time has occured since 1/1/00 so that you can figure out how much time is between observations\n",
    "pos_sec_since = []\n",
    "jan1_2007 = datetime(2007,1,1)\n",
    "for i in range(0,len(arm_pos_data)):\n",
    "    pos_sec_since.append((arm_pos_data['date'][i]-jan1_2007).total_seconds())\n",
    "arm_pos_data['sec_since']=pos_sec_since\n",
    "\n",
    "arm_pos_data_daily = arm_pos_data.set_index('date').groupby(pd.Grouper(freq='d')).mean() #calculate daily data\n",
    "arm_pos_data_daily = arm_pos_data_daily.reset_index() #reset the index after you claculate daily data\n",
    "\n",
    "distance=[float('nan')]\n",
    "obs_duration_days=[float('nan')]\n",
    "for i in range(1,len(arm_pos_data_daily)):\n",
    "    distance.append(np.sqrt((arm_pos_data_daily['UTM_EASTING'][i]-arm_pos_data_daily['UTM_EASTING'][i-1])**2+(arm_pos_data_daily['UTM_NORTHING'][i]-arm_pos_data_daily['UTM_NORTHING'][i-1])**2))   \n",
    "    obs_duration_days.append((arm_pos_data_daily['sec_since'][i]-arm_pos_data_daily['sec_since'][i-1])/3600/24)   \n",
    "arm_pos_data_daily['Distance']=distance\n",
    "arm_pos_data_daily['obs_duration_days']=obs_duration_days\n",
    "arm_pos_data_daily[['Distance','obs_duration_days']] = arm_pos_data_daily[['Distance','obs_duration_days']].apply(pd.to_numeric)#change the data types\n",
    "arm_pos_data_daily['Velocity_m_per_d']=arm_pos_data_daily['Distance']/arm_pos_data_daily['obs_duration_days']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate velocity between two dates:\n",
    "\n",
    "# Whyjay project\n",
    "start_dates = ['2018-05-08','2018-09-03', '2018-08-18', '2018-08-02', '2018-04-28', '2018-04-12','2018-05-23', '2018-04-21', '2018-04-05', '2018-03-04','2018-09-30', '2018-09-20', '2018-09-10', '2018-08-31', '2018-08-11', '2018-08-01', '2018-07-27', '2018-07-22', '2018-06-27', '2018-06-12', '2018-05-23', '2018-05-18', '2018-05-08', '2018-03-29', '2018-03-14', '2018-03-04','2018-09-17', '2018-08-18', '2018-07-29', '2018-07-24', '2018-07-04', '2018-06-19', '2018-05-15', '2018-03-16', '2018-03-06']\n",
    "end_dates = ['2018-06-27','2018-10-05', '2018-09-03', '2018-08-18', '2018-08-02', '2018-04-28','2018-06-08', '2018-05-23', '2018-04-21', '2018-04-05','2018-10-05', '2018-09-30', '2018-09-20', '2018-09-10', '2018-08-31', '2018-08-11', '2018-08-01', '2018-07-27', '2018-07-22', '2018-06-27', '2018-06-12', '2018-05-23', '2018-05-18', '2018-05-08', '2018-03-29', '2018-03-14','2018-10-02', '2018-09-17', '2018-08-18', '2018-07-29', '2018-07-24', '2018-07-04', '2018-06-19', '2018-05-15', '2018-03-16']\n",
    "\n",
    "\n",
    "for i in range(0,len(start_dates)):\n",
    "    vel_date_start=pd.to_datetime(start_dates[i],format='%Y-%m-%d')\n",
    "    vel_date_end=pd.to_datetime(end_dates[i],format='%Y-%m-%d')\n",
    "\n",
    "    #Lower\n",
    "    for i in range(0,len(lower_pos_data_daily)):\n",
    "        if lower_pos_data_daily['date'][i]==vel_date_start:\n",
    "            start_pos= lower_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "        if lower_pos_data_daily['date'][i]==vel_date_end:\n",
    "            end_pos=lower_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "\n",
    "    distance_traveled = np.sqrt((start_pos['UTM_EASTING']-end_pos['UTM_EASTING'])**2+(start_pos['UTM_NORTHING']-end_pos['UTM_NORTHING'])**2)\n",
    "    time_between_obs = (end_pos['sec_since']-start_pos['sec_since'])/3600/24\n",
    "\n",
    "    velocity_obs_lower=pd.DataFrame({'date1':[vel_date_start],'date2':[vel_date_end],'start_easting':[start_pos['UTM_EASTING']],'start_northing':[start_pos['UTM_NORTHING']],'end_easting':[end_pos['UTM_EASTING']],'end_northing':[end_pos['UTM_NORTHING']],'distance_traveled (m)':[distance_traveled],'velocity (m/d)':[distance_traveled/time_between_obs]},index=[0])\n",
    "    velocity_obs_lower['label']='lower'\n",
    "    \n",
    "    #Middle\n",
    "    for i in range(0,len(middle_pos_data_daily)):\n",
    "        if middle_pos_data_daily['date'][i]==vel_date_start:\n",
    "            start_pos= middle_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "        if middle_pos_data_daily['date'][i]==vel_date_end:\n",
    "            end_pos=middle_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "\n",
    "    distance_traveled = np.sqrt((start_pos['UTM_EASTING']-end_pos['UTM_EASTING'])**2+(start_pos['UTM_NORTHING']-end_pos['UTM_NORTHING'])**2)\n",
    "    time_between_obs = (end_pos['sec_since']-start_pos['sec_since'])/3600/24\n",
    "\n",
    "    velocity_obs_middle=pd.DataFrame({'date1':[vel_date_start],'date2':[vel_date_end],'start_easting':[start_pos['UTM_EASTING']],'start_northing':[start_pos['UTM_NORTHING']],'end_easting':[end_pos['UTM_EASTING']],'end_northing':[end_pos['UTM_NORTHING']],'distance_traveled (m)':[distance_traveled],'velocity (m/d)':[distance_traveled/time_between_obs]},index=[1])\n",
    "    velocity_obs_middle['label']='middle'\n",
    "    \n",
    "    #Upper\n",
    "    for i in range(0,len(upper_pos_data_daily)):\n",
    "        if upper_pos_data_daily['date'][i]==vel_date_start:\n",
    "            start_pos= upper_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "        if upper_pos_data_daily['date'][i]==vel_date_end:\n",
    "            end_pos=upper_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "\n",
    "    distance_traveled = np.sqrt((start_pos['UTM_EASTING']-end_pos['UTM_EASTING'])**2+(start_pos['UTM_NORTHING']-end_pos['UTM_NORTHING'])**2)\n",
    "    time_between_obs = (end_pos['sec_since']-start_pos['sec_since'])/3600/24\n",
    "\n",
    "    velocity_obs_upper=pd.DataFrame({'date1':[vel_date_start],'date2':[vel_date_end],'start_easting':[start_pos['UTM_EASTING']],'start_northing':[start_pos['UTM_NORTHING']],'end_easting':[end_pos['UTM_EASTING']],'end_northing':[end_pos['UTM_NORTHING']],'distance_traveled (m)':[distance_traveled],'velocity (m/d)':[distance_traveled/time_between_obs]},index=[2])\n",
    "    velocity_obs_upper['label']='upper'\n",
    "    \n",
    "    #Arm\n",
    "    for i in range(0,len(arm_pos_data_daily)):\n",
    "        if arm_pos_data_daily['date'][i]==vel_date_start:\n",
    "            start_pos= arm_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "        if arm_pos_data_daily['date'][i]==vel_date_end:\n",
    "            end_pos=arm_pos_data_daily[['date','UTM_EASTING','UTM_NORTHING','HGT(m)','sec_since']].iloc[i]\n",
    "\n",
    "    distance_traveled = np.sqrt((start_pos['UTM_EASTING']-end_pos['UTM_EASTING'])**2+(start_pos['UTM_NORTHING']-end_pos['UTM_NORTHING'])**2)\n",
    "    time_between_obs = (end_pos['sec_since']-start_pos['sec_since'])/3600/24\n",
    "\n",
    "    velocity_obs_arm=pd.DataFrame({'date1':[vel_date_start],'date2':[vel_date_end],'start_easting':[start_pos['UTM_EASTING']],'start_northing':[start_pos['UTM_NORTHING']],'end_easting':[end_pos['UTM_EASTING']],'end_northing':[end_pos['UTM_NORTHING']],'distance_traveled':[distance_traveled],'velocity':[distance_traveled/time_between_obs]},index=[3])\n",
    "    velocity_obs_arm['label']='arm'\n",
    "    \n",
    "    # print(velocity_obs_lower)\n",
    "    # print(velocity_obs_middle)\n",
    "    # print(velocity_obs_upper)\n",
    "    # print(velocity_obs_arm)\n",
    "\n",
    "#     velocity_data_between_two_dates = pd.concat([velocity_obs_lower,velocity_obs_middle,velocity_obs_upper],axis=0)\n",
    "    velocity_data_between_two_dates = pd.concat([velocity_obs_lower,velocity_obs_middle,velocity_obs_upper,velocity_obs_arm],axis=0)\n",
    "\n",
    "    csv_name='Kaskawulsh_v2_'+vel_date_start.strftime('%Y-%m-%d')+'_to_'+vel_date_end.strftime('%Y-%m-%d')+'_GPS.csv'\n",
    "    velocity_data_between_two_dates.to_csv(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2938c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd6d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b93af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b79892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c317e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed429b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b3144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1eb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174322f7-1efd-4afa-976a-575a1823c812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
